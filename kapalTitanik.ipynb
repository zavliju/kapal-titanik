{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=r'C:\\Users\\Zavli\\Desktop\\train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path=r'C:\\Users\\Zavli\\Desktop\\test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = sc.textFile(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rdd = sc.textFile(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S',\n",
       " '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C',\n",
       " '3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S',\n",
       " '4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q',\n",
       " '893,3,\"Wilkes, Mrs. James (Ellen Needs)\",female,47,1,0,363272,7,,S',\n",
       " '894,2,\"Myles, Mr. Thomas Francis\",male,62,0,0,240276,9.6875,,Q',\n",
       " '895,3,\"Wirz, Mr. Albert\",male,27,0,0,315154,8.6625,,S']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseTrain(rdd):\n",
    "\n",
    "\t# ekstrak data header (row satu)\n",
    "    header = rdd.first()\n",
    "\t# remove header\n",
    "    body = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "    def parseRow(row):\n",
    "\t\t# fungsi untuk Parse RDD ke DataFrame \n",
    "\n",
    "\t\t# remove double quote, split teks row dengan comma\n",
    "        row_list = row.replace('\"','').split(\",\")\n",
    "\t\t# konversi python list ke tuple, yang kompetibel dengan struktur data pyspark \n",
    "        row_tuple = tuple(row_list)\n",
    "        return row_tuple\n",
    "\n",
    "    rdd_parsed = body.map(parseRow)\n",
    "\n",
    "    colnames = header.split(\",\")\n",
    "    colnames.insert(3,'FirstName')\n",
    "\n",
    "    return rdd_parsed.toDF(colnames)\n",
    "\n",
    "def parseTest(rdd):\n",
    "    header = rdd.first()\n",
    "    body = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "    def parseRow(row):\n",
    "        row_list = row.replace('\"','').split(\",\")\n",
    "        row_tuple = tuple(row_list)\n",
    "        return row_tuple\n",
    "\n",
    "    rdd_parsed = body.map(parseRow)\n",
    "    \n",
    "    colnames = header.split(\",\")\n",
    "    colnames.insert(2,'FirstName')\n",
    "\n",
    "    return rdd_parsed.toDF(colnames)\n",
    "\n",
    "train_df = parseTrain(train_rdd)\n",
    "test_df = parseTest(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|FirstName|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|   Braund|     Mr. Owen Harris|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|  Cumings| Mrs. John Bradle...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen|         Miss. Laina|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add kolom survived ke data test \n",
    "from pyspark.sql.functions import lit, col\n",
    "train_df = train_df.withColumn('Mark',lit('train'))\n",
    "test_df = (test_df.withColumn('Survived',lit(0))\n",
    "                  .withColumn('Mark',lit('test')))\n",
    "test_df = test_df[train_df.columns]\n",
    "## Append data test ke data train \n",
    "df = train_df.unionAll(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[PassengerId: string, Survived: string, Pclass: string, FirstName: string, Name: string, Sex: string, Age: string, SibSp: string, Parch: string, Ticket: string, Fare: string, Cabin: string, Embarked: string, Mark: string]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+----+\n",
      "|PassengerId|Survived|Pclass|   FirstName|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Mark|\n",
      "+-----------+--------+------+------------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+----+\n",
      "|        892|       0|     3|       Kelly|           Mr. James|  male|34.5|    0|    0|          330911| 7.8292|     |       Q|test|\n",
      "|        893|       0|     3|      Wilkes| Mrs. James (Elle...|female|  47|    1|    0|          363272|      7|     |       S|test|\n",
      "|        894|       0|     2|       Myles|  Mr. Thomas Francis|  male|  62|    0|    0|          240276| 9.6875|     |       Q|test|\n",
      "|        895|       0|     3|        Wirz|          Mr. Albert|  male|  27|    0|    0|          315154| 8.6625|     |       S|test|\n",
      "|        896|       0|     3|    Hirvonen| Mrs. Alexander (...|female|  22|    1|    1|         3101298|12.2875|     |       S|test|\n",
      "|        897|       0|     3|    Svensson|    Mr. Johan Cervin|  male|  14|    0|    0|            7538|  9.225|     |       S|test|\n",
      "|        898|       0|     3|    Connolly|          Miss. Kate|female|  30|    0|    0|          330972| 7.6292|     |       Q|test|\n",
      "|        899|       0|     2|    Caldwell|  Mr. Albert Francis|  male|  26|    1|    1|          248738|     29|     |       S|test|\n",
      "|        900|       0|     3|     Abrahim| Mrs. Joseph (Sop...|female|  18|    0|    0|            2657| 7.2292|     |       C|test|\n",
      "|        901|       0|     3|      Davies|     Mr. John Samuel|  male|  21|    2|    0|       A/4 48871|  24.15|     |       S|test|\n",
      "|        902|       0|     3|      Ilieff|            Mr. Ylio|  male|    |    0|    0|          349220| 7.8958|     |       S|test|\n",
      "|        903|       0|     1|       Jones| Mr. Charles Cresson|  male|  46|    0|    0|             694|     26|     |       S|test|\n",
      "|        904|       0|     1|      Snyder| Mrs. John Pillsb...|female|  23|    1|    0|           21228|82.2667|  B45|       S|test|\n",
      "|        905|       0|     2|      Howard|        Mr. Benjamin|  male|  63|    1|    0|           24065|     26|     |       S|test|\n",
      "|        906|       0|     1|     Chaffee| Mrs. Herbert Ful...|female|  47|    1|    0|     W.E.P. 5734| 61.175|  E31|       S|test|\n",
      "|        907|       0|     2|   del Carlo| Mrs. Sebastiano ...|female|  24|    1|    0|   SC/PARIS 2167|27.7208|     |       C|test|\n",
      "|        908|       0|     2|       Keane|          Mr. Daniel|  male|  35|    0|    0|          233734|  12.35|     |       Q|test|\n",
      "|        909|       0|     3|       Assaf|          Mr. Gerios|  male|  21|    0|    0|            2692|  7.225|     |       C|test|\n",
      "|        910|       0|     3|  Ilmakangas|    Miss. Ida Livija|female|  27|    1|    0|STON/O2. 3101270|  7.925|     |       S|test|\n",
      "|        911|       0|     3|Assaf Khalil| Mrs. Mariana (Mi...|female|  45|    0|    0|            2696|  7.225|     |       C|test|\n",
      "+-----------+--------+------+------------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: double (nullable = true)\n",
      " |-- Parch: double (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Mark: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Data Cleaning/Manipulation\n",
    "## konversi tipe data Age, SibSp, Parch, Fare ke double\n",
    "df = (df.withColumn('Age',df['Age'].cast(\"double\"))\n",
    "\t\t\t.withColumn('SibSp',df['SibSp'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Parch',df['Parch'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Fare',df['Fare'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Survived',df['Survived'].cast(\"double\"))\n",
    "\t\t\t)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impute missing Age dan Fare\n",
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "def countNull(df,var):\n",
    "\treturn df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {var: countNull(df,var) for var in numVars}\n",
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Name|Title|\n",
      "+--------------------+-----+\n",
      "|     Mr. Owen Harris|   Mr|\n",
      "| Mrs. John Bradle...|  Mrs|\n",
      "|         Miss. Laina| Miss|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "## fungsi untuk mengekstrak title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(),StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|Embarked|Embarked_indexed|\n",
      "+--------+----------------+\n",
      "|       S|             0.0|\n",
      "|       C|             1.0|\n",
      "|       S|             0.0|\n",
      "+--------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catVars = ['Pclass','Sex','Embarked','Title']\n",
    " \n",
    "## index variabel Sex \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')\n",
    " \n",
    "## menggunakan pipeline untuk mengindex semua kategori variabel\n",
    "def indexer(df,col):\n",
    "    si = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df)\n",
    "    return si\n",
    " \n",
    "indexers = [indexer(df,col) for col in catVars]\n",
    " \n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    " \n",
    "df_indexed.select('Embarked','Embarked_indexed').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+-----+\n",
      "| mark|label|            features|index|\n",
      "+-----+-----+--------------------+-----+\n",
      "|train|  0.0|[22.0,1.0,0.0,7.2...|  0.0|\n",
      "|train|  1.0|[38.0,1.0,0.0,71....|  1.0|\n",
      "|train|  1.0|[26.0,0.0,0.0,7.9...|  1.0|\n",
      "+-----+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catVarsIndexed = [i+'_indexed' for i in catVars]\n",
    "featuresCol = numVars+catVarsIndexed\n",
    "featuresCol.remove('Survived')\n",
    "labelCol = ['Mark','Survived']\n",
    " \n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "row = Row('mark','label','features')\n",
    " \n",
    "df_indexed = df_indexed[labelCol+featuresCol]\n",
    "# 0-mark, 1-label, 2-features\n",
    "# map features ke DenseVector\n",
    "lf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],DenseVector(r[2:])))).toDF()\n",
    "# index label\n",
    "# mengonversi label numerik ke ketegori yang dibutuhkan untuk dt dan rf\n",
    "lf = StringIndexer(inputCol = 'label',outputCol='index').fit(lf).transform(lf)\n",
    " \n",
    "lf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 636\n",
      "Validate Data: 255\n",
      "Test Data: 418\n"
     ]
    }
   ],
   "source": [
    "train = lf.where(lf.mark =='train')\n",
    "test = lf.where(lf.mark =='test')\n",
    " \n",
    "# random split yang untuk mendapakan train/validate\n",
    "train,validate = train.randomSplit([0.7,0.3],seed =121)\n",
    " \n",
    "print ('Train Data: '+ str(train.count()))\n",
    "print ('Validate Data: '+ str(validate.count()))\n",
    "print ('Test Data: '+ str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logsitic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regPara: regualrization parameter\n",
    "#lr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)\n",
    "\n",
    "# Menggunakan model evaluasi auc ROC(default buat binary classification)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def testModel(model, validate = validate):\n",
    "    pred = model.transform(validate)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n",
    "    return evaluator.evaluate(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT dan RF\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "#dt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\n",
    "#rf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n",
    "#models = {'LogisticRegression':lr,\n",
    "#          'DecistionTree':dt,\n",
    "#          'RandomForest':rf}\n",
    " \n",
    "#modelPerf = {k:testModel(v) for k,v in models.iteritems()}\n",
    " \n",
    "#print (modelPerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
